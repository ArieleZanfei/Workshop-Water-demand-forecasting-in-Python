{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973b9e66",
   "metadata": {},
   "source": [
    "# Improve the sustainable management of water networks: develop a water demand forecasting algorithm with neural networks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0407c6b",
   "metadata": {},
   "source": [
    "![alt text](MLP.png \"Title\")\n",
    "\n",
    "What could we do with the knowledge of our water network future consumption? Surely, we can consistently improve and optimize the way we use water. For instance, knowing the consumption of the following day would permit an optimal selection of the pump operation, with consequent important savings in terms of energy and money. This is one of the reasons why the development of reliable water demand forecasting algorithms is a really hot topic in the scientific community. Clearly, water demand forecasting is not an easy task due the strong stochastic behavior of our consumption, which depend mainly by our habits. However, in the latest decades, we have a new ally for the development of such algorithm: the increase of data availability. Therefore, during this workshop we will see how we can develop a powerful algorithm for predicting the future water demand of a water network, based on the data-driven idea that is at the basis of the machine and deep learning philosophy. Therefore, we will take some data and create and ANN algorithm step-by-step using Python, analyzing the results and the methodology. Hence the workshop will be developed as follows:\n",
    "- Introduction to the importance of sustainability of water networks, and on the new challenge of today.\n",
    "- Discuss the idea of water demand forecasting and give a closer look to the data-driven idea behind machine learning algorithms.\n",
    "- Develop a step-by-step code that, starting form some data analysis, will arrive at the development of a neural network model for predicting water demand time series.\n",
    "- Analyze the output, and discuss the potential of such approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52cf190",
   "metadata": {},
   "source": [
    "### Time to import some foundamental libraries of Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c185a985",
   "metadata": {},
   "source": [
    "### Now lets import our data and take a look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b02bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('demand_ts.csv', sep=';')\n",
    "df.index = pd.to_datetime(df['index'],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df.drop(columns=df.columns[[0]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde91e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d346faf",
   "metadata": {},
   "source": [
    "## Always important to see graphically what is inside our data\n",
    "The below code will allow us to see the whole time series content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator(interval=6))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m')) \n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.plot(df, linewidth=1.5, label = 'Hourly data')   \n",
    "ax.plot(df.resample('D').mean(), linewidth=1.5, label = 'Daily data') \n",
    "ax.set_ylabel('(L/s)', fontsize=22)\n",
    "ax.legend()\n",
    "ax.grid(True)       \n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime.strptime('2013-04-01', '%Y-%m-%d')\n",
    "end_date = start_date + datetime.timedelta(days = 2)\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator(byhour=0))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%y-%m-%d')) \n",
    "ax.format_xdata = mdates.DateFormatter('%y-%m-%d')\n",
    "ax.plot(df[start_date:end_date], linewidth=1.5)\n",
    "ax.set_ylabel('(L/s)', fontsize=22)\n",
    "ax.grid(True)          \n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c333d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "plot_pacf(df, lags=200, method='ywm', ax=ax)\n",
    "ax.set_xlabel('lag', fontsize=22)\n",
    "ax.set_title('')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d08dc",
   "metadata": {},
   "source": [
    "## Past observation are our best ally\n",
    "This analysis allows us to understand the autocorrelations that existis inside our data. Yes, past observation are our best ally during the prediction task. Therefore, our next step is to manipulate the data carefully in order to prepare them for the neural network model. How? We will use some coding skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_selector(dataset, lista_lag):\n",
    "    dataset_copy = dataset.copy()\n",
    "    n_lags = len(lista_lag)\n",
    "    n = dataset_copy.index.size\n",
    "    for i in range(n_lags):\n",
    "        if i == 0:\n",
    "            lag = lista_lag[i]\n",
    "            df_lag = dataset_copy.copy()\n",
    "            for j in range(lag):\n",
    "                dataset = dataset.drop(dataset.index[[0]])\n",
    "                df_lag = df_lag.drop(df_lag.index[[n-j-1]])  \n",
    "            counter = 0\n",
    "            for col in dataset_copy.columns:\n",
    "                name = col + '_lag'+str(lag)\n",
    "                dataset[name] = df_lag.values[:, counter]\n",
    "                counter = counter + 1\n",
    "        else:\n",
    "            lag = lista_lag[i]\n",
    "            lag0 = lista_lag[0]\n",
    "            df_lag = dataset_copy.copy()\n",
    "            for j in range(lag0-lag):\n",
    "                df_lag = df_lag.drop(df_lag.index[[0]])\n",
    "            for j in range(lag):\n",
    "                a = df_lag.index.size\n",
    "                df_lag = df_lag.drop(df_lag.index[[a-1]])  \n",
    "            counter = 0\n",
    "            for col in dataset_copy.columns:\n",
    "                name = col + '_lag'+str(lag)\n",
    "                dataset[name] = df_lag.values[:, counter]\n",
    "                counter = counter + 1\n",
    "    n_var = dataset.columns.size\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset_multi = lag_selector(dataset = df,\n",
    "                            lista_lag = [24, 23, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d134b",
   "metadata": {},
   "source": [
    "The method above is made to compose the dataset in columns, where each columns is the same type of variable, but referred to a different time step. For instance, the value in the column lag3 refers to 3 time step (in this case 3 hours) before the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d93892",
   "metadata": {},
   "source": [
    "### We need also scaler\n",
    "Neural netwroks wants data in a precise manner. Shortly, it is better to provide the data in a range between 0 and 1. What is usually dane is to scale the dataset with a min max scaler. Take a look below.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =  MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(dataset_multi.values)\n",
    "# need this for later\n",
    "scaler_univ =  MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_univ = scaler_univ.fit_transform(df.values)\n",
    "\n",
    "# take a look\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88774d51",
   "metadata": {},
   "source": [
    "Now we have to create a function to carefully prepare and divide our data into a sample for the training of the model, and a sample for testing the model. Why? It is a well accepted way to build a data-driven model. We can use a portion of our model to tune the hyperparameter of our data-driven, in this case the ANN. The remainig data are needed to see if what we did was correct, hence if our model was fitted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe84958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_splitter(scaled_dataset, split_point):\n",
    "    \n",
    "    seq_to_seq_dataset = scaled_dataset[range(0, scaled_dataset.shape[0], 1), :]\n",
    "    # i look only to the dimension of the first col\n",
    "    n_train_hours = int(round(scaled_dataset[:,0].size*split_point)) \n",
    "    # divide\n",
    "    train = seq_to_seq_dataset[:n_train_hours, :]\n",
    "    test = seq_to_seq_dataset[n_train_hours:, :]\n",
    "# =========================================================================\n",
    "    train_X, train_y = train[:, 1:], train[:, 0:1]            \n",
    "    test_X, test_y = test[:, 1:], test[:, 0:1]\n",
    "    #reshape input . \n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    train_y = train_y.reshape((train_y.shape[0], 1, train_y.shape[1]))\n",
    "    train_y = train_y[:,:,::-1]\n",
    "    test_y = test_y.reshape((test_y.shape[0], 1, test_y.shape[1]))\n",
    "    test_y = test_y[:,:,::-1]\n",
    "                \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "train_X, train_y, test_X, test_y = test_train_splitter(scaled_dataset = scaled, split_point=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "ax.plot(range(train_y[:,0,0].size) , train_y[:,0,0], linewidth=1.5, label = 'training')   \n",
    "ax.plot(range(train_y[:,0,0].size, scaled[:,0].size), test_y[:,0,0], linewidth=1.5, label = 'testing') \n",
    "#ax.set_ylabel('(L/s)', fontsize=22)\n",
    "ax.grid(True)       \n",
    "ax.legend(fontsize=18)\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55546c38",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "This is a very important part. Normally, we should pay particular attention to the creation of the model, and we should choose wisely the typology of the model (we could even use a different neural network, like a recurrent one) and also on the hyperparameter selection. Which optimizes shall we use? how many layers? how many neurons? These question can be solved with experince and with search methods like a grid search. In other words, usually we should test a lot of model configuration to find the best one. For this time, we use a not optimised deep neural network, knowing that we could improve our work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()      \n",
    "model.add(Dense(96, input_shape=(train_X.shape[1],train_X.shape[2]),\n",
    "                     activation='relu'))  \n",
    "model.add(Dense(96, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf1802b",
   "metadata": {},
   "source": [
    "## Fit our model\n",
    "Now we have to fit our model on our data. This means that the model is trained over our training sample and its weights are adjusted in order to make the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ca0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse', \n",
    "              optimizer = 'Adam', \n",
    "              metrics = ['accuracy'])              \n",
    "hist = model.fit(train_X, train_y, epochs=20, \n",
    "                  batch_size=24, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8add18e",
   "metadata": {},
   "source": [
    "## Let's take a look at the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656f424",
   "metadata": {},
   "source": [
    "## Now we can use the model to make the preditictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(train_X)\n",
    "trainPredict = np.fliplr(trainPredict)\n",
    "testPredict = model.predict(test_X)\n",
    "testPredict = np.fliplr(testPredict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13591740",
   "metadata": {},
   "source": [
    "## Remember that we scaled everything. We need to revert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93151e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into original\n",
    "train_y = train_y.reshape((train_y.shape[0], 1))\n",
    "test_y = test_y.reshape((test_y.shape[0], 1))\n",
    "train_y = scaler_univ.inverse_transform(np.fliplr(train_y))\n",
    "test_y = scaler_univ.inverse_transform(np.fliplr(test_y))\n",
    "real_concat = np.concatenate((train_y, test_y), axis=None) \n",
    "# for predictions\n",
    "trainPredict = trainPredict.reshape((trainPredict.shape[0], 1))\n",
    "testPredict = testPredict.reshape((testPredict.shape[0], 1))\n",
    "trainPredict = scaler_univ.inverse_transform(trainPredict)\n",
    "testPredict = scaler_univ.inverse_transform(testPredict)\n",
    "model_concat = np.concatenate((trainPredict, testPredict), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d258f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = pd.DataFrame(index=range(dataset_multi.index.size), \n",
    "                            columns=['h average'])\n",
    "df_forcasted = pd.DataFrame(index=range(dataset_multi.index.size), \n",
    "                                 columns=['h average'])\n",
    "df_train = pd.DataFrame(index=range(dataset_multi.index.size),\n",
    "                             columns=['h average'])\n",
    "df_test = pd.DataFrame(index=range(dataset_multi.index.size), \n",
    "                            columns=['h average'])\n",
    "\n",
    "df_real['h average'] = real_concat\n",
    "df_forcasted['h average'] = model_concat\n",
    "df_train['h average'][0:len(trainPredict)] = model_concat[0:len(trainPredict)]\n",
    "df_test['h average'][len(trainPredict):] = model_concat[len(trainPredict):]\n",
    "\n",
    "#ricreo l'indice\n",
    "index = pd.date_range(start = df.index[0],\n",
    "                      periods = dataset_multi.index.size, freq='H')\n",
    "df_forcasted.index = index\n",
    "df_real.index = index\n",
    "df_train.index = index \n",
    "df_test.index = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ea2af",
   "metadata": {},
   "source": [
    "# Let's take a look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "#ax.plot(range(train_y[:,0].size) , train_y[:,0], linewidth=1.5, label = 'training')   \n",
    "#ax.plot(range(train_y[:,0].size, scaled[:,0].size), test_y[:,0], linewidth=1.5, label = 'testing') \n",
    "\n",
    "ax.plot(df_train, linewidth=1.5, label = 'trainPredict', alpha=0.6)   \n",
    "ax.plot(df_test, linewidth=1.5, label = 'testPredict', alpha=0.6) \n",
    "ax.plot(df_real, linewidth=1.5, label = 'real', alpha=0.3, color='k')\n",
    "ax.set_ylabel('(L/s)', fontsize=22)\n",
    "ax.grid(True)       \n",
    "ax.legend(fontsize=18)\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a72db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime.strptime(('2013-09-01'), '%Y-%m-%d')\n",
    "end_date = start_date + datetime.timedelta(days = 10)\n",
    "fig, (ax) = plt.subplots(1, figsize=(15, 7))\n",
    "ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator(interval=1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H'))\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.plot(df_real[start_date:end_date], label = 'True')\n",
    "ax.plot(df_train[start_date:end_date])\n",
    "ax.plot(df_test[start_date:end_date], label = 'prediction')\n",
    "ax.set_ylabel('(L/s)')\n",
    "ax.set_title('Hourly Forecasting')\n",
    "ax.legend()\n",
    "# ax.legend(('real data','training','validation'))\n",
    "ax.grid(True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d989a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef421d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
